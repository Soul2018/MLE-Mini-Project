{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1m+nuWgQwkVmQPV7MjEPg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Soul2018/MLE-Mini-Project/blob/main/MLE_Mini_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "#Load the dataset into a pandas DataFrame\n",
        "df = pd.read_parquet('/content/yellow_tripdata_2025-01.parquet')\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(df.head(5))\n",
        "\n",
        "# Drop rows with missing values.\n",
        "df = df.dropna()\n",
        "\n",
        "# Create new feature, 'trip_duration\n",
        "  #here we copy of the DataFrame in order to avoid SettingWithCopyWarning\n",
        "df = df.copy()\n",
        "df['trip_duration'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
        "\n",
        "print(df[['trip_duration']].head())\n",
        "\n",
        "# Create a list called feature_col to store column names\n",
        "feature_cols = df.columns.to_list()\n",
        "print(feature_cols)\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "\n",
        "  #let's define the feature and target\n",
        "X = df.drop(columns=['fare_amount'])\n",
        "y = df['fare_amount']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training set shape\", X_train.shape, y_train.shape)\n",
        "print(\"Test set shape\", X_test.shape, y_test.shape)\n",
        "\n",
        "# Create a baseline for mean absolute error of total amount\n",
        "# let's create a model that always predicts the mean total fare of the training dataset\n",
        "\n",
        "mean_total_fare = y_train.mean();\n",
        "class PredictMean:\n",
        "  def predicator(self, X):\n",
        "    return np.full(shape=(len(X),), fill_value=mean_total_fare)\n",
        "# model instantiation\n",
        "mean_model = PredictMean();\n",
        "\n",
        "# let's make predictions on test data\n",
        "y_predict = mean_model.predicator(X_test)\n",
        "print(\"Predicted mean on test set\", y_predict)\n",
        "\n",
        "#now we can calculate the mean absolute error\n",
        "mean_absolute_err = mean_absolute_error(y_test, y_predict)\n",
        "\n",
        "print(\"Mean absolute error: \", mean_absolute_err)\n",
        "\n",
        "# Use Scikit-Learn's ColumnTransformer to preprocess the categorical and\n",
        "# continuous features independently.\n",
        "\n",
        "#let's define the categorical features\n",
        "categorical_features = [\"store_and_fwd_flag\"]\n",
        "#let's define the continue features\n",
        "continuous_features = [\"tpep_dropoff_datetime\"]\n",
        "\n",
        "# Create a pipeline for categorical features\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "# create a pipeline for continuous features\n",
        "continuous_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "# let's combine the pipelines\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('category', categorical_transformer, categorical_features),\n",
        "        (\"continuous\", continuous_transformer, continuous_features)\n",
        "    ]\n",
        ")\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_predict = pipeline.predict(X_test)\n",
        "\n",
        "print(\"Predicted value\", y_predict[:5])\n",
        "\n",
        "# Build random forest regressor model\n",
        "\n",
        "#let's create a Randon Forest model instance\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# trainig model\n",
        "# Convert datetime columns to numerical features\n",
        "X_train['pickup_hour'] = X_train['tpep_pickup_datetime'].dt.hour\n",
        "X_train['dropoff_hour'] = X_train['tpep_dropoff_datetime'].dt.hour\n",
        "X_train['trip_duration'] = (X_train['tpep_dropoff_datetime'] - X_train['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
        "\n",
        "X_test['pickup_hour'] = X_test['tpep_pickup_datetime'].dt.hour\n",
        "X_test['dropoff_hour'] = X_test['tpep_dropoff_datetime'].dt.hour\n",
        "X_test['trip_duration'] = (X_test['tpep_dropoff_datetime'] - X_test['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
        "\n",
        "# Drop the original datetime columns\n",
        "X_train = X_train.drop(columns=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\n",
        "X_test = X_test.drop(columns=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\n",
        "\n",
        "# One-hot encode the 'store_and_fwd_flag' column\n",
        "X_train = pd.get_dummies(X_train, columns=['store_and_fwd_flag'], drop_first=True)  \n",
        "X_test = pd.get_dummies(X_test, columns=['store_and_fwd_flag'], drop_first=True)  \n",
        "\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# predict on test data set\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "#mean absolute error\n",
        "mean_absolute_err_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "\n",
        "print(\"Random Forest Regressor Mean Absolute Error:\", mean_absolute_err_rf)"
      ],
      "metadata": {
        "id": "mR4NAfa69QNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "#Load the dataset into a pandas DataFrame\n",
        "df = pd.read_parquet('/content/yellow_tripdata_2025-01.parquet')\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(df.head(5))\n",
        "\n",
        "# Drop rows with missing values.\n",
        "df = df.dropna()\n",
        "\n",
        "# Create new feature, 'trip_duration\n",
        "  #here we copy of the DataFrame in order to avoid SettingWithCopyWarning\n",
        "df = df.copy()\n",
        "df['pickup_hour'] = df['tpep_pickup_datetime'].dt.hour\n",
        "df['dropoff_hour'] = df['tpep_dropoff_datetime'].dt.hour\n",
        "df['trip_duration'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
        "\n",
        "print(df[['trip_duration']].head())\n",
        "\n",
        "# Create a list called feature_col to store column names\n",
        "feature_cols = df.columns.to_list()\n",
        "print(feature_cols)\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "\n",
        "  #let's define the feature and target\n",
        "X = df.drop(columns=['fare_amount'])\n",
        "y = df['fare_amount']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training set shape\", X_train.shape, y_train.shape)\n",
        "print(\"Test set shape\", X_test.shape, y_test.shape)\n",
        "\n",
        "# Create a baseline for mean absolute error of total amount\n",
        "# let's create a model that always predicts the mean total fare of the training dataset\n",
        "\n",
        "mean_total_fare = y_train.mean();\n",
        "class PredictMean:\n",
        "  def predicator(self, X):\n",
        "    return np.full(shape=(len(X),), fill_value=mean_total_fare)\n",
        "# model instantiation\n",
        "mean_model = PredictMean();\n",
        "\n",
        "# let's make predictions on test data\n",
        "y_predict = mean_model.predicator(X_test)\n",
        "print(\"Predicted mean on test set\", y_predict)\n",
        "\n",
        "#now we can calculate the mean absolute error\n",
        "mean_absolute_err = mean_absolute_error(y_test, y_predict)\n",
        "\n",
        "print(\"Mean absolute error: \", mean_absolute_err)\n",
        "\n",
        "# Use Scikit-Learn's ColumnTransformer to preprocess the categorical and\n",
        "# continuous features independently.\n",
        "\n",
        "#let's define the categorical features\n",
        "categorical_features = [\"store_and_fwd_flag\"]\n",
        "#let's define the continue features\n",
        "continuous_features = [\"tpep_dropoff_datetime\"]\n",
        "\n",
        "# Create a pipeline for categorical features\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "# create a pipeline for continuous features\n",
        "continuous_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "# let's combine the pipelines\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('category', categorical_transformer, categorical_features),\n",
        "        (\"continuous\", continuous_transformer, continuous_features)\n",
        "    ]\n",
        ")\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_predict = pipeline.predict(X_test)\n",
        "\n",
        "print(\"Predicted value\", y_predict[:5])\n",
        "\n",
        "# Build random forest regressor model\n",
        "\n",
        "#let's create a Randon Forest model instance\n",
        "rf_model = RandomForestRegressor(n_estimators=10, random_state=42, max_depth=2)\n",
        "\n",
        "# trainig model\n",
        "\n",
        "# Drop the original datetime columns\n",
        "X_train = X_train.drop(columns=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\n",
        "X_test = X_test.drop(columns=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\n",
        "\n",
        "# One-hot encode the 'store_and_fwd_flag' column\n",
        "X_train = pd.get_dummies(X_train, columns=['store_and_fwd_flag'], drop_first=True)\n",
        "X_test = pd.get_dummies(X_test, columns=['store_and_fwd_flag'], drop_first=True)\n",
        "\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# predict on test data set\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "#mean absolute error\n",
        "mean_absolute_err_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "\n",
        "print(\"Random Forest Regressor Mean Absolute Error:\", mean_absolute_err_rf)\n",
        "\n",
        "# Define the hyperparameters to tune.\n",
        "param_grid = {\n",
        "    'n_estimators': [10],\n",
        "    'max_depth': [2],\n",
        "    'min_samples_split': [2],\n",
        "    'min_samples_leaf': [1],\n",
        "    'max_features': ['auto'],\n",
        "    'bootstrap': [True]\n",
        "}\n",
        "# Perform grid search to find the best hyperparameters. This could take a while\n",
        "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid,\n",
        "                           scoring='neg_mean_absolute_error', cv=3, verbose=2, n_jobs=-1)"
      ],
      "metadata": {
        "id": "0GQ5wdrqBYhT",
        "outputId": "fe4fac09-ec14-488a-956b-897deda20599",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
            "0         1  2025-01-01 00:18:38   2025-01-01 00:26:59              1.0   \n",
            "1         1  2025-01-01 00:32:40   2025-01-01 00:35:13              1.0   \n",
            "2         1  2025-01-01 00:44:04   2025-01-01 00:46:01              1.0   \n",
            "3         2  2025-01-01 00:14:27   2025-01-01 00:20:01              3.0   \n",
            "4         2  2025-01-01 00:21:34   2025-01-01 00:25:06              3.0   \n",
            "\n",
            "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
            "0           1.60         1.0                  N           229           237   \n",
            "1           0.50         1.0                  N           236           237   \n",
            "2           0.60         1.0                  N           141           141   \n",
            "3           0.52         1.0                  N           244           244   \n",
            "4           0.66         1.0                  N           244           116   \n",
            "\n",
            "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
            "0             1         10.0    3.5      0.5        3.00           0.0   \n",
            "1             1          5.1    3.5      0.5        2.02           0.0   \n",
            "2             1          5.1    3.5      0.5        2.00           0.0   \n",
            "3             2          7.2    1.0      0.5        0.00           0.0   \n",
            "4             2          5.8    1.0      0.5        0.00           0.0   \n",
            "\n",
            "   improvement_surcharge  total_amount  congestion_surcharge  Airport_fee  \n",
            "0                    1.0         18.00                   2.5          0.0  \n",
            "1                    1.0         12.12                   2.5          0.0  \n",
            "2                    1.0         12.10                   2.5          0.0  \n",
            "3                    1.0          9.70                   0.0          0.0  \n",
            "4                    1.0          8.30                   0.0          0.0  \n",
            "   trip_duration\n",
            "0       8.350000\n",
            "1       2.550000\n",
            "2       1.950000\n",
            "3       5.566667\n",
            "4       3.533333\n",
            "['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'Airport_fee', 'pickup_hour', 'dropoff_hour', 'trip_duration']\n",
            "Training set shape (2348061, 21) (2348061,)\n",
            "Test set shape (587016, 21) (587016,)\n",
            "Predicted mean on test set [17.61466675 17.61466675 17.61466675 ... 17.61466675 17.61466675\n",
            " 17.61466675]\n",
            "Mean absolute error:  11.366645494225901\n",
            "Predicted value [17.76396912 17.36565927 17.33276016 17.32057008 17.4257387 ]\n",
            "Random Forest Regressor Mean Absolute Error: 4.311853773518298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3B8qPmoICeJo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}