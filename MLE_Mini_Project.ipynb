{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAl02QI+RYD3wSx1dHR5we",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Soul2018/MLE-Mini-Project/blob/main/MLE_Mini_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "#Load the dataset into a pandas DataFrame\n",
        "df = pd.read_parquet('/content/yellow_tripdata_2022-01.parquet')\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(df.head(5))\n",
        "\n",
        "# Drop rows with missing values.\n",
        "df = df.dropna()\n",
        "\n",
        "# Create new feature, 'trip_duration\n",
        "  #here we copy of the DataFrame in order to avoid SettingWithCopyWarning\n",
        "df = df.copy()\n",
        "df['pickup_hour'] = df['tpep_pickup_datetime'].dt.hour\n",
        "df['dropoff_hour'] = df['tpep_dropoff_datetime'].dt.hour\n",
        "df['trip_duration'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
        "\n",
        "print(df[['trip_duration']].head())\n",
        "\n",
        "# Create a list called feature_col to store column names\n",
        "feature_cols = df.columns.to_list()\n",
        "print(feature_cols)\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "\n",
        "  #let's define the feature and target\n",
        "X = df.drop(columns=['fare_amount'])\n",
        "y = df['fare_amount']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training set shape\", X_train.shape, y_train.shape)\n",
        "print(\"Test set shape\", X_test.shape, y_test.shape)\n",
        "\n",
        "# Create a baseline for mean absolute error of total amount\n",
        "# let's create a model that always predicts the mean total fare of the training dataset\n",
        "\n",
        "mean_total_fare = y_train.mean();\n",
        "class PredictMean:\n",
        "  def predicator(self, X):\n",
        "    return np.full(shape=(len(X),), fill_value=mean_total_fare)\n",
        "# model instantiation\n",
        "mean_model = PredictMean();\n",
        "\n",
        "# let's make predictions on test data\n",
        "y_predict = mean_model.predicator(X_test)\n",
        "print(\"Predicted mean on test set\", y_predict)\n",
        "\n",
        "#now we can calculate the mean absolute error\n",
        "mean_absolute_err = mean_absolute_error(y_test, y_predict)\n",
        "\n",
        "print(\"Mean absolute error: \", mean_absolute_err)\n",
        "\n",
        "# Use Scikit-Learn's ColumnTransformer to preprocess the categorical and\n",
        "# continuous features independently.\n",
        "\n",
        "#let's define the categorical features\n",
        "categorical_features = [\"store_and_fwd_flag\"]\n",
        "#let's define the continue features\n",
        "continuous_features = [\"tpep_dropoff_datetime\"]\n",
        "\n",
        "# Create a pipeline for categorical features\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "# create a pipeline for continuous features\n",
        "continuous_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "# let's combine the pipelines\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('category', categorical_transformer, categorical_features),\n",
        "        (\"continuous\", continuous_transformer, continuous_features)\n",
        "    ]\n",
        ")\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_predict = pipeline.predict(X_test)\n",
        "\n",
        "print(\"Predicted value\", y_predict[:5])\n",
        "\n",
        "# Build random forest regressor model\n",
        "\n",
        "#let's create a Randon Forest model instance\n",
        "rf_model = RandomForestRegressor(n_estimators=10, random_state=42, max_depth=2)\n",
        "\n",
        "# trainig model\n",
        "\n",
        "# Drop the original datetime columns\n",
        "X_train = X_train.drop(columns=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\n",
        "X_test = X_test.drop(columns=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\n",
        "\n",
        "# One-hot encode the 'store_and_fwd_flag' column\n",
        "X_train = pd.get_dummies(X_train, columns=['store_and_fwd_flag'], drop_first=True)\n",
        "X_test = pd.get_dummies(X_test, columns=['store_and_fwd_flag'], drop_first=True)\n",
        "\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# predict on test data set\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "#mean absolute error\n",
        "mean_absolute_err_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "\n",
        "print(\"Random Forest Regressor Mean Absolute Error:\", mean_absolute_err_rf)\n",
        "\n",
        "# Define the hyperparameters to tune.\n",
        "param_grid = {\n",
        "    'n_estimators': [10, 50],\n",
        "    'max_depth': [2, 5]\n",
        "}\n",
        "#let's define an new instance\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Perform grid search to find the best hyperparameters. This could take a while\n",
        "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid,\n",
        "                           scoring='neg_mean_absolute_error', cv=2, verbose=2, n_jobs=-1)\n",
        "\n",
        "#let's reduce the execution time using subset data\n",
        "\n",
        "X_train_subset = X_train.head(1000)\n",
        "y_train_subset = y_train.head(1000)\n",
        "\n",
        "\n",
        "# Fit the grid search to the training data.\n",
        "grid_search.fit(X_train_subset, y_train_subset)\n",
        "\n",
        "# Get the best model.\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Get the best models' hyperparameter.\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Fit the best classifier on the training data.\n",
        "best_model.fit(X_train_subset, y_train_subset)\n",
        "\n",
        "# Make predictions on the test data set\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "\n",
        "print(\"Predicted value\", y_pred_best[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8j-6JSpiPfHg",
        "outputId": "75f75c44-e3d2-4f54-f28d-1399acec32dc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
            "0         1  2022-01-01 00:35:40   2022-01-01 00:53:29              2.0   \n",
            "1         1  2022-01-01 00:33:43   2022-01-01 00:42:07              1.0   \n",
            "2         2  2022-01-01 00:53:21   2022-01-01 01:02:19              1.0   \n",
            "3         2  2022-01-01 00:25:21   2022-01-01 00:35:23              1.0   \n",
            "4         2  2022-01-01 00:36:48   2022-01-01 01:14:20              1.0   \n",
            "\n",
            "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
            "0           3.80         1.0                  N           142           236   \n",
            "1           2.10         1.0                  N           236            42   \n",
            "2           0.97         1.0                  N           166           166   \n",
            "3           1.09         1.0                  N           114            68   \n",
            "4           4.30         1.0                  N            68           163   \n",
            "\n",
            "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
            "0             1         14.5    3.0      0.5        3.65           0.0   \n",
            "1             1          8.0    0.5      0.5        4.00           0.0   \n",
            "2             1          7.5    0.5      0.5        1.76           0.0   \n",
            "3             2          8.0    0.5      0.5        0.00           0.0   \n",
            "4             1         23.5    0.5      0.5        3.00           0.0   \n",
            "\n",
            "   improvement_surcharge  total_amount  congestion_surcharge  airport_fee  \n",
            "0                    0.3         21.95                   2.5          0.0  \n",
            "1                    0.3         13.30                   0.0          0.0  \n",
            "2                    0.3         10.56                   0.0          0.0  \n",
            "3                    0.3         11.80                   2.5          0.0  \n",
            "4                    0.3         30.30                   2.5          0.0  \n",
            "   trip_duration\n",
            "0      17.816667\n",
            "1       8.400000\n",
            "2       8.966667\n",
            "3      10.033333\n",
            "4      37.533333\n",
            "['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'airport_fee', 'pickup_hour', 'dropoff_hour', 'trip_duration']\n",
            "Training set shape (1913942, 21) (1913942,)\n",
            "Test set shape (478486, 21) (478486,)\n",
            "Predicted mean on test set [12.84439731 12.84439731 12.84439731 ... 12.84439731 12.84439731\n",
            " 12.84439731]\n",
            "Mean absolute error:  7.555914948890032\n",
            "Predicted value [11.56674957 13.05640338 11.59495894 12.7872885  12.95951881]\n",
            "Random Forest Regressor Mean Absolute Error: 3.5794759434419343\n",
            "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n",
            "Best Hyperparameters: {'max_depth': 5, 'n_estimators': 50}\n",
            "Predicted value [ 9.12494189  7.45236272  9.90824676 20.82317167  8.57506503]\n"
          ]
        }
      ]
    }
  ]
}